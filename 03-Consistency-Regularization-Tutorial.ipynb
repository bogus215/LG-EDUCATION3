{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f04a7",
   "metadata": {},
   "source": [
    "# Consistency regularization Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4643f4",
   "metadata": {},
   "source": [
    "- 실습조교: 배진수(wlstn215@korea.ac.kr), 안시후(sihuahn@korea.ac.kr), 김현지(99ktxx@korea.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcf9ad",
   "metadata": {},
   "source": [
    "## Colab gpu 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317be656",
   "metadata": {},
   "source": [
    "### 런타임 -> 런타임유형 변경 -> 하드웨어 가속기 -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c448520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835011df",
   "metadata": {},
   "source": [
    "## 0.모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' github+colab 교육생분들 '''\n",
    "# !git clone https://github.com/bogus215/LG-EDUCATION3.git\n",
    "\n",
    "''' 기본 모듈 및 시각화 모듈 '''\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import easydict # dictionary의 속성을 dot(.)을 사용하여 표기가능\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "'''Neural Network을 위한 딥러닝 모듈'''\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''Custom 모듈 (WIdeResNet, RandAugment)'''\n",
    "''' github+colab 교육생분들 '''\n",
    "# import sys\n",
    "# sys.path.append(\"./LG-EDUCATION3\")\n",
    "from models import Wide_ResNet, initialize_weights\n",
    "from augmentation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0a8c3",
   "metadata": {},
   "source": [
    "## 1. 준지도학습 분석 환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 셋팅\n",
    "args = easydict.EasyDict({\n",
    "    \n",
    "    # base configuration for learning\n",
    "    \"seed\": 0,\n",
    "    \"gpu\": 0,\n",
    "    \"lambda_u\" : 20,\n",
    "    \"total_epoch\" : 10,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\",\n",
    "    \"num_labeled\" : 10000,\n",
    "    \"num_unlabeled\" : 40000,\n",
    "    \"num_classes\" : 10,\n",
    "    \"size\" : 32,\n",
    "    \"batch_size\" : 256,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10, \n",
    "    \"widen_factor\" : 2, \n",
    "    \"dropout\" : 0.1,\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.02, # train learning rate of teacher model\n",
    "    \"weight_decay\" : 2e-4, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 고정\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 할당하기\n",
    "args.device = torch.device('cuda', args.gpu) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bd8a5",
   "metadata": {},
   "source": [
    "## 2.데이터셋 & 데이터로더 준비 (Labeled & Unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 데이터 다운로드\n",
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 별 데이터 개수\n",
    "labeled_per_class = args.num_labeled // args.num_classes\n",
    "unlabeled_per_class = args.num_unlabeled // args.num_classes \n",
    "\n",
    "print(\"클래스 1개당 Labeled 데이터 개수: \",labeled_per_class)\n",
    "print(\"클래스 1개당 Unlabeled 데이터 개수: \",unlabeled_per_class)\n",
    "\n",
    "# FULL DATA (Labeled & Unlabeled)\n",
    "images, labels = np.array(base_dataset.data), np.array(base_dataset.targets)\n",
    "\n",
    "labeled_idx, unlabeled_idx = [], []\n",
    "for i in range(args.num_classes):\n",
    "    class_idx = np.where(labels==i)[0]\n",
    "    labeled_idx.extend(class_idx[:labeled_per_class])\n",
    "    unlabeled_idx.extend(class_idx[labeled_per_class:(labeled_per_class+unlabeled_per_class)])\n",
    "   \n",
    "# Labeled 데이터 총 갯수 확인하기\n",
    "print(\"Labeled 데이터 총 갯수: \",len(labeled_idx))\n",
    "\n",
    "# Unlabeled 데이터 총 갯수 확인하기\n",
    "print(\"Unabeled 데이터 총 갯수: \",len(unlabeled_idx))\n",
    "\n",
    "labeled_images, unlabeled_images = images[labeled_idx], images[unlabeled_idx]\n",
    "labeled_targets, unlabeled_targets = labels[labeled_idx], labels[unlabeled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_labeled_images, valid_labeled_images, train_labeled_targets, valid_labeled_targets = train_test_split(labeled_images, labeled_targets, test_size=.2, stratify=labeled_targets)\n",
    "\n",
    "print(train_labeled_images.shape)\n",
    "print(valid_labeled_images.shape)\n",
    "print(train_labeled_targets.shape)\n",
    "print(valid_labeled_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandAugment에서 사용할 augmentation 리스트 생성\n",
    "# Augmentation별 강도 조절 및 환경과 관련된 하이퍼파라미터 설정 필요\n",
    "cifar10_aug_list = [\n",
    "                    (Color, 4, 0.5),\n",
    "                    (Contrast, 4, 0.5),\n",
    "                    (Brightness, 4, 0.5),\n",
    "                    (Sharpness, 4, 0.5),\n",
    "                    ]\n",
    "\n",
    "# RandAugment 클래스 만들기\n",
    "class RandAugmentCIFAR(object):\n",
    "    def __init__(self, n, m, aug_list):\n",
    "\n",
    "        self.n = int(n)\n",
    "        self.m = m\n",
    "        self.augment_pool = aug_list\n",
    "\n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_pool, k=self.n)\n",
    "        for op, max_v, bias in ops:\n",
    "            img = op(img, v=self.m, max_v=max_v, bias=bias)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용될 평균, 표준편차\n",
    "cifar10_mean = [0.49139968, 0.48215841, 0.44653091]\n",
    "cifar10_std = [0.24703223, 0.24348513, 0.26158784]\n",
    "\n",
    "# print(base_dataset.data.mean(axis=(0,1,2))/255)\n",
    "# print(base_dataset.data.std(axis=(0,1,2))/255)\n",
    "\n",
    "# Labeled 데이터셋을 위한 데이터변환\n",
    "transform_labeled = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomCrop(size=args.size, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "            ])\n",
    "\n",
    "# Test 데이터셋을 위한 데이터변환\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "            ])\n",
    "\n",
    "# Unlabeled 데이터셋을 위한 데이터변환\n",
    "class CustomTransform(object):\n",
    "\n",
    "    def __init__(self, args, n, m, mean, std, aug_list):\n",
    "        self.n, self.m = n, m\n",
    "        \n",
    "        self.weak_aug = transforms.Compose([transforms.ToPILImage(),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.RandomCrop(size=args.size, padding=4)\n",
    "                                            ])\n",
    "        \n",
    "        self.strong_aug = transforms.Compose([transforms.ToPILImage(),\n",
    "                                              RandAugmentCIFAR(n=n, m=m, aug_list=aug_list)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=mean, std=std)])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        weak_augmented = self.weak_aug(x)\n",
    "        strong_augmented = self.strong_aug(x)\n",
    "\n",
    "        return self.normalize(weak_augmented), self.normalize(strong_augmented)\n",
    "    \n",
    "transform_unlabeled = CustomTransform(args, n=1, m=1, mean=cifar10_mean, std=cifar10_std, aug_list=cifar10_aug_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 데이터셋 구축하기\n",
    "class CIFAR10SSL(Dataset):\n",
    "    def __init__(self, X, Y, transform):\n",
    "        \n",
    "        self.x_data = X\n",
    "        self.targets = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img, target = self.x_data[index], self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 데이터 로더 구축 완료\n",
    "train_labeled_dataset = CIFAR10SSL(X=train_labeled_images, Y = train_labeled_targets, transform=transform_labeled)\n",
    "valid_labeled_dataset = CIFAR10SSL(X=valid_labeled_images, Y = valid_labeled_targets, transform=transform_test)\n",
    "\n",
    "unlabeled_dataset = CIFAR10SSL(X=unlabeled_images, Y=unlabeled_targets, transform=transform_unlabeled)\n",
    "test_dataset = CIFAR10SSL(X=np.array(test_dataset.data),Y=np.array(test_dataset.targets),transform=transform_test)\n",
    "\n",
    "train_labeled_loader = DataLoader(train_labeled_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "valid_labeled_loader = DataLoader(valid_labeled_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "## 3.Semi-supervised learning (Consistency regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wide_ResNet(num_classes=args.num_classes,depth=args.depth,widen_factor=args.widen_factor,dropout_rate=args.dropout)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=args.lr, momentum=0.9, nesterov=True, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca96ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "best_loss = np.inf\n",
    "\n",
    "# 모델은 training mode로 설정\n",
    "for epoch in (pbar :=tqdm(range(args.total_epoch))):\n",
    "\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    for ind, (label_inputs, label_targets) in enumerate(train_labeled_loader):\n",
    "\n",
    "        label_inputs = label_inputs.to(args.device)\n",
    "        label_targets = label_targets.to(args.device)\n",
    "\n",
    "        try:\n",
    "            (weak_inputs, strong_inputs), _ = next(unlabeled_iter)\n",
    "            weak_inputs = weak_inputs.to(args.device)\n",
    "            strong_inputs = strong_inputs.to(args.device)\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (weak_inputs, strong_inputs), _ = next(unlabeled_iter)\n",
    "            weak_inputs = weak_inputs.to(args.device)        \n",
    "            strong_inputs = strong_inputs.to(args.device)\n",
    "            \n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        full_logits = model(torch.cat([label_inputs,weak_inputs, strong_inputs],axis=0))\n",
    "        label_logits, weak_unlabeled_logits, strong_unlabeled_logits = full_logits.split(label_targets.size(0))\n",
    "        label_loss = criterion(label_logits, label_targets)\n",
    "\n",
    "        unlabeled_loss = F.mse_loss(input=strong_unlabeled_logits.softmax(-1), target=weak_unlabeled_logits.softmax(-1).detach())\n",
    "        tot_loss = args.lambda_u*unlabeled_loss + label_loss\n",
    "        \n",
    "        # backward\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch별 loss를 축적함\n",
    "        epoch_loss += tot_loss.item()\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        preds = label_logits.argmax(dim=-1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == label_targets).item()\n",
    "        total += label_targets.size(0)        \n",
    "\n",
    "        pbar.set_description(f'Loss : {tot_loss.item():.2f}  |  Accuracy: {torch.sum(preds == label_targets).item()/label_targets.size(0):.2f}')\n",
    "        \n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss /= len(train_labeled_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    pbar.set_description(f'{epoch+1} Loss : {(epoch_loss)/(ind+1):.2f}  |  Accuracy : {(corrects/total):.2f}')\n",
    "    \n",
    "\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        for ind, (inputs, targets) in enumerate(valid_labeled_loader):\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # batch별 loss를 축적함\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets).item()\n",
    "            total += targets.size(0)        \n",
    "\n",
    "            pbar.set_description(f'Loss : {loss.item():.2f}  |  Accuracy: {torch.sum(preds == targets).item()/targets.size(0):.2f}')\n",
    "\n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss /= len(valid_labeled_loader)\n",
    "    valid_loss.append(epoch_loss)\n",
    "    pbar.set_description(f'{epoch+1} Loss : {(epoch_loss)/(ind+1):.2f}  |  Accuracy : {(corrects/total):.2f}')\n",
    "    \n",
    "    if best_loss > valid_loss[-1]:\n",
    "        print(best_loss)\n",
    "        best_loss = valid_loss[-1]\n",
    "        torch.save(model.state_dict(), \"CA-1.pth\")\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"CA-1.pth\"))\n",
    "model.eval()\n",
    "corrects = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        preds = outputs.argmax(-1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == targets).item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "test_acc = corrects / total\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e75179",
   "metadata": {},
   "source": [
    "## 4.Semi-supervised learning (Consistency regularization + Sharpening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & optimizer 설정\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=args.lr, momentum=0.9, nesterov=True, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "best_loss = np.inf\n",
    "\n",
    "# 모델은 training mode로 설정\n",
    "for epoch in (pbar :=tqdm(range(args.total_epoch))):\n",
    "\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    for ind, (label_inputs, label_targets) in enumerate(train_labeled_loader):\n",
    "\n",
    "        label_inputs = label_inputs.to(args.device)\n",
    "        label_targets = label_targets.to(args.device)\n",
    "\n",
    "        try:\n",
    "            (weak_inputs, strong_inputs), _ = next(unlabeled_iter)\n",
    "            weak_inputs = weak_inputs.to(args.device)\n",
    "            strong_inputs = strong_inputs.to(args.device)\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (weak_inputs, strong_inputs), _ = next(unlabeled_iter)\n",
    "            weak_inputs = weak_inputs.to(args.device)        \n",
    "            strong_inputs = strong_inputs.to(args.device)\n",
    "            \n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        full_logits = model(torch.cat([label_inputs,weak_inputs, strong_inputs],axis=0))\n",
    "        label_logits, weak_unlabeled_logits, strong_unlabeled_logits = full_logits.split(label_targets.size(0))\n",
    "        label_loss = criterion(label_logits, label_targets)\n",
    "\n",
    "        unlabeled_loss = F.mse_loss(input=strong_unlabeled_logits.softmax(-1), target=((weak_unlabeled_logits.softmax(-1))**2 / (((weak_unlabeled_logits.softmax(-1))**2).sum(-1).reshape(-1,1))).detach())\n",
    "        tot_loss = args.lambda_u*unlabeled_loss + label_loss\n",
    "        \n",
    "        # backward\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch별 loss를 축적함\n",
    "        epoch_loss += tot_loss.item()\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        preds = label_logits.argmax(dim=-1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == label_targets).item()\n",
    "        total += label_targets.size(0)        \n",
    "\n",
    "        pbar.set_description(f'Loss : {tot_loss.item():.2f}  |  Accuracy: {torch.sum(preds == label_targets).item()/label_targets.size(0):.2f}')\n",
    "        \n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss /= len(train_labeled_loader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    pbar.set_description(f'{epoch+1} Loss : {(epoch_loss)/(ind+1):.2f}  |  Accuracy : {(corrects/total):.2f}')\n",
    "    \n",
    "\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        for ind, (inputs, targets) in enumerate(valid_labeled_loader):\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # batch별 loss를 축적함\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets).item()\n",
    "            total += targets.size(0)        \n",
    "\n",
    "            pbar.set_description(f'Loss : {loss.item():.2f}  |  Accuracy: {torch.sum(preds == targets).item()/targets.size(0):.2f}')\n",
    "\n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss /= len(valid_labeled_loader)\n",
    "    valid_loss.append(epoch_loss)\n",
    "    pbar.set_description(f'{epoch+1} Loss : {(epoch_loss)/(ind+1):.2f}  |  Accuracy : {(corrects/total):.2f}')\n",
    "    \n",
    "    if best_loss > valid_loss[-1]:\n",
    "        print(best_loss)\n",
    "        best_loss = valid_loss[-1]\n",
    "        torch.save(model.state_dict(), \"CA-2.pth\")\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"CA-2.pth\"))\n",
    "model.eval()\n",
    "corrects = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        preds = outputs.argmax(-1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == targets).item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "test_acc = corrects / total\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_, axs = plt.subplots(2,1,figsize=(10,5))\n",
    "\n",
    "axs[0].bar(x=np.arange(10),height=weak_unlabeled_logits.softmax(-1)[0].detach().cpu().numpy(), alpha=0.5)\n",
    "axs[0].set_title(\"before\")\n",
    "axs[0].set_ylim(0,1)\n",
    "\n",
    "\n",
    "target=((weak_unlabeled_logits.softmax(-1))**2 / (((weak_unlabeled_logits.softmax(-1))**2).sum(-1).reshape(-1,1))).detach()\n",
    "axs[1].bar(x=np.arange(10),height=target[0].cpu().numpy(), alpha=.5)\n",
    "axs[1].set_title(\"after\")\n",
    "axs[1].set_ylim(0,1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
